{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include \"/home/ubuntu/commitgen/model/utils.lua\"\n",
    "local Decoder, parent = torch.class('Decoder', 'nn.Module')\n",
    "include \"/home/ubuntu/commitgen/model/MaskedLoss.lua\"\n",
    "\n",
    "function Decoder:__init(config, params)\n",
    "\tparent.__init(self)\n",
    "\tself.emb_dim = config.emb_dim\n",
    "\tself.emb_out = config.emb_out\n",
    "\tself.params = params\n",
    "\n",
    "\tself.cell = self:cell()\n",
    "  self.paramx, self.paramdx = self.cell:getParameters()\n",
    "  self.paramx:uniform(-self.params.init_weight, self.params.init_weight)\n",
    "\tself.paramdx:zero()\n",
    "\n",
    "\tself.cells = cloneManyTimes(self.cell, self.params.max_nl_length)\n",
    "\n",
    "\tself.s={}\n",
    "\tself.ds={}\n",
    "\n",
    "\tfor j = 0, self.params.max_nl_length do\n",
    "\t\tself.s[j] = {}\n",
    "\t\tfor d = 1, 2 * self.params.layers do\n",
    "\t\t\tself.s[j][d] = torch.zeros(self.params.batch_size, self.params.rnn_size):cuda()\n",
    "\t\tend\n",
    "\tend\n",
    "\tself.cudaones = torch.ones(1):cuda()\n",
    "\n",
    "\tfor d = 1, 2 * self.params.layers do\n",
    "\t\tself.ds[d] = torch.zeros(self.params.batch_size, self.params.rnn_size):cuda()\n",
    "\tend\n",
    "\n",
    "end\n",
    "\n",
    "function Decoder:cell()\n",
    "\n",
    "\t  -- decoder with attention cell\n",
    "\t  -- with position input\n",
    "\n",
    "\t  local  all_h = nn.Identity()()\n",
    "\t  local infmask = nn.Identity()()\n",
    "\n",
    "\t  -- inffmask is seq x minibatch\n",
    "\n",
    "\n",
    "    -- regular decoder cell\n",
    "\t  local x                = nn.Identity()()\n",
    " \t\tlocal y                = nn.Identity()()\n",
    "\t\tlocal prev_s           = nn.Identity()()\n",
    "\n",
    "\t\tlocal pos              = nn.Identity()()\n",
    "\t\tlocal pos_lookup       = nn.LookupTable(101, 50)(pos)\n",
    "\n",
    "\t\tlocal i                = {[0] = nn.JoinTable(2)({pos_lookup, nn.LookupTable(self.emb_dim, self.emb_out)(x)}) }\n",
    "\n",
    "\t\tlocal next_s           = {}\n",
    "\t\tlocal splitted         = {prev_s:split(2 * self.params.layers)}\n",
    "\t\tfor layer_idx = 1, self.params.layers do\n",
    "\t\t\tlocal prev_c         = splitted[2 * layer_idx - 1]\n",
    "\t\t\tlocal prev_h         = splitted[2 * layer_idx]\n",
    "\t\t\tlocal dropped        = nn.Dropout(self.params.dropout)(i[layer_idx - 1])\n",
    "\t\t\tlocal next_c, next_h = lstm(dropped, prev_c, prev_h, 450) -- self.emb_out)\n",
    "\t\t\ttable.insert(next_s, next_c)\n",
    "\t\t\ttable.insert(next_s, next_h)\n",
    "\t\t\ti[layer_idx] = next_h\n",
    "\t\tend\n",
    "\n",
    "\n",
    "\t\t-- ok, now we have the decoder state in i[1]\n",
    "    -- size of all_h is  minibatch x seq x 400\n",
    "    -- size of dropped in minibatch x 400 \n",
    "    -- we want minibatch x seq x 1\n",
    "    -- Do not use log soft max here.. since we have to take a weighted combination\n",
    "    -- using these weights\n",
    "    -- Remember that before doing the softmax, we have to convert those weights\n",
    "    -- which correspond to the padding to -infinity so that softmax assigns them \n",
    "    -- a weight of zero\n",
    "    --\n",
    "    local attention_weights = nn.Select(3, 1)(nn.MM()({all_h, nn.Replicate(1, 3)(i[self.params.layers])}))\n",
    "\n",
    "\n",
    "    local masked_attention_weights = nn.SoftMax()(nn.CAddTable()({attention_weights, nn.Transpose({1,2})(infmask)}))\n",
    "\n",
    "\t\t-- attention is minibatch x seq --> make this minibatch x seq x 1\n",
    "\t\t-- all_h is minibatch x seq x 400\n",
    "\t\t-- we want minibatch x 400 x 1\n",
    "\t\t-- Then reduce it to minibatch x 400\n",
    "\t\t\n",
    "    local context_vector = nn.Select(3, 1)(nn.MM(true, false)({all_h, nn.Replicate(1, 3)(masked_attention_weights)})):annotate{name = 'ATTENTION'}\n",
    "\n",
    "\n",
    " \t\tlocal W1 = nn.Linear(self.params.rnn_size, self.params.rnn_size)\n",
    " \t\tlocal W2 = nn.Linear(self.params.rnn_size, self.params.rnn_size)\n",
    "\n",
    "\t\t-- The final vector. We then softmax this\n",
    " \t\tlocal h_att = nn.Tanh()(nn.CAddTable()({W1(i[self.params.layers]), W2(context_vector)}))\n",
    "\n",
    "\t\tlocal dropped          = nn.Dropout(self.params.dropout)(h_att)\n",
    "\n",
    " \t\tlocal h2y              = nn.Linear(self.params.rnn_size, self.emb_dim)\n",
    " \t\tlocal pred             = nn.LogSoftMax()(h2y(dropped))\n",
    " \t\tlocal err              = MaskedLoss()({pred, y})\n",
    "\n",
    "    return  nn.gModule({x, y, pos, prev_s, all_h, infmask}, {err, nn.Identity()(next_s)}):cuda()\n",
    "end\n",
    "\n",
    "function Decoder:forward(state, batch, all_h) \n",
    "\n",
    "\tfor i = 1, (batch.maxY - 1) do\n",
    "\t  err, self.s[i] = unpack(self.cells[i]:forward({batch.y[i], batch.y[i + 1], torch.ones(self.params.batch_size):cuda() * i, self.s[i - 1], all_h, batch.infmask}))\n",
    "\n",
    "\t\tstate.count = state.count + err[2]\n",
    "\t\tstate.normal = state.normal + err[3]\n",
    "\tend\n",
    "\tstate.acc = state.count / state.normal\n",
    "end\n",
    "\n",
    "function Decoder:backward(batch, all_h)\n",
    "\tfor d = 1, #self.ds do\n",
    "\t\tself.ds[d]:zero()\n",
    "\tend\n",
    "\n",
    "\tlocal sum_d_all_h = all_h:clone():zero()\n",
    "\n",
    "\tfor i = (batch.maxY - 1), 1, -1 do\n",
    "\n",
    "\t\tlocal d_prev, d_next, d_s, d_all_h, d_mask\n",
    "\t  d_prev, d_next, d_pos, d_s, d_all_h, d_mask = unpack(self.cells[i]:backward(\n",
    "\t\t\t{batch.y[i], batch.y[i + 1], torch.ones(self.params.batch_size):cuda() * i, self.s[i - 1], all_h, batch.infmask},\n",
    "\t\t\t{self.cudaones, self.ds}))\n",
    " \t\t  sum_d_all_h:add(d_all_h)\n",
    "\n",
    "\t\tcopy_table(self.ds, d_s)\n",
    "\tend\n",
    "\n",
    "\t-- we also need to return the d's that have added up for the encoder\n",
    "\treturn sum_d_all_h\n",
    "\n",
    "end\n",
    "\n",
    "function Decoder:training()\n",
    "\tg_enable_dropout_all(self.cells, params.dropout)\n",
    "end\n",
    "\n",
    "function Decoder:evaluate()\n",
    "\tg_disable_dropout_all(self.cells)\n",
    "end\n",
    "\n",
    "function Decoder:updateParameters()\n",
    "  update_params(self.paramx, self.paramdx, self.params.max_grad_norm, self.params.learningRate, self.params.normalize)\n",
    "\tself.paramdx:zero()\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    gpu=1,\n",
    "    layers=1,\n",
    "    rnn_size=400,\n",
    "    learningRate=0.1,\n",
    "    max_grad_norm=5,\n",
    "    init_weight=0.5,\n",
    "    dropout=0.5,\n",
    "    decay=0.8,\n",
    "    normalize=1,\n",
    "    max_length=20,\n",
    "    beam_size=10,\n",
    "    max_nl_length=100,\n",
    "\tmax_code_length=100,\n",
    "\tbatch_size=100,\n",
    "}\n",
    "\n",
    "decoder = Decoder({emb_dim=3, emb_out=4}, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#decoder.cell.forwardnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61\t\n",
       "{\n",
       "  annotations : \n",
       "    {\n",
       "      _debugLabel : [[string \"include \"/home/ubuntu/commitgen/model/utils.l...\"]]:89_\n",
       "    }\n",
       "  module : \n",
       "    nn.MM\n",
       "    {\n",
       "      _type : torch.CudaTensor\n",
       "      output : CudaTensor - empty\n",
       "      gradInput : \n",
       "        {\n",
       "          1 : CudaTensor - empty\n",
       "          2 : CudaTensor - empty\n",
       "        }\n",
       "      transA : true\n",
       "      transB : false\n",
       "    }\n",
       "  reverseMap : \n",
       "    {\n",
       "      nngraph.Node : true\n",
       "    }\n",
       "  forwardNodeId : 46\n",
       "  mapindex : \n",
       "    {\n",
       "      1 : \n",
       "        {\n",
       "          annotations : \n",
       "            {\n",
       "              _debugLabel : [[string \"include \"/home/ubuntu/commitgen/model/utils.l...\"]]:40_\n",
       "            }\n",
       "          module : \n",
       "            nn.Identity\n",
       "            {\n",
       "              gradInput : CudaTensor - empty\n",
       "              _type : torch.CudaTensor\n",
       "              output : CudaTensor - empty\n",
       "            }\n",
       "          reverseMap : \n",
       "            {\n",
       "              nngraph.Node : true\n",
       "              nngraph.Node : true\n",
       "            }\n",
       "          forwardNodeId : 49\n",
       "          mapindex : \n",
       "            {\n",
       "              1 : \n",
       "                {\n",
       "                  annotations : table: 0x40cd2490\n",
       "                  input : table: 0x40cd2468\n",
       "                  mapindex : table: 0x40cd2668\n",
       "                  selectindex : 5\n",
       "                  forwardNodeId : 53\n",
       "                  reverseMap : table: 0x40cd2690\n",
       "                }\n",
       "              table: 0x40cd2440 : 1\n",
       "            }\n",
       "        }\n",
       "      2 : \n",
       "        {\n",
       "          annotations : \n",
       "            {\n",
       "              _debugLabel : [[string \"include \"/home/ubuntu/commitgen/model/utils.l...\"]]:89_\n",
       "            }\n",
       "          module : \n",
       "            nn.Replicate\n",
       "            {\n",
       "              nfeatures : 1\n",
       "              _type : torch.CudaTensor\n",
       "              output : CudaTensor - empty\n",
       "              gradInput : CudaTensor - empty\n",
       "              dim : 3\n",
       "            }\n",
       "          reverseMap : \n",
       "            {\n",
       "              nngraph.Node : true\n",
       "            }\n",
       "          forwardNodeId : 50\n",
       "          mapindex : \n",
       "            {\n",
       "              1 : \n",
       "                {\n",
       "                  annotations : table: 0x40e41530\n",
       "                  module : nn.SoftMax\n",
       "                  reverseMap : table: 0x40e41580\n",
       "                  forwardNodeId : 54\n",
       "                  mapindex : table: 0x40e41558\n",
       "                }\n",
       "              table: 0x40e41318 : 1\n",
       "            }\n",
       "        }\n",
       "      table: 0x40e41d78 : 2\n",
       "      table: 0x4011fc20 : 1\n",
       "    }\n",
       "}\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.cell.forwardnodes[50].data.mapindex[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decoder.cell.forwardnodes[43].data.mapindex[1].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "JSON = (loadfile \"commitgen/model/JSON.lua\")() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value = nil\n",
    "tensors = {}\n",
    "for line in io.lines(\"/tmp/lua_SsOb8o\") do\n",
    "    if #line > 0 then\n",
    "        if value == nil then\n",
    "            value = line\n",
    "        else\n",
    "            value = value .. \"\\n\" .. line\n",
    "        end\n",
    "    else\n",
    "        -- print(value)\n",
    "        t = torch.deserialize(value, \"ascii\")\n",
    "        table.insert(tensors,t)\n",
    "        value=\"\"\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = torch.totable(tensors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.039528504014015,0.039528504014015,0.062131494283676,0.044466331601143,0.018017549067736,0.030503196641803,0.035669844597578],[0.034676723182201,0.034676723182201,0.039691116660833,0.025204978883266,0.030837003141642,0.018189964815974,0.02582111954689],[0.0019375256961212,0.0019375256961212,0.0012629881966859,0.0043391864746809,0.0018298089271411,0.0011462797410786,0.002435308881104],[0.056111969053745,0.056111969053745,0.067134208977222,0.042234148830175,0.030622810125351,0.035720609128475,0.029433626681566],[0.0045833084732294,0.0045833084732294,0.0027174670249224,0.0048443539999425,0.0031658185180277,0.0013434329302981,0.0045913867652416],[0.051353637129068,0.051353637129068,0.04918610304594,0.050211042165756,0.033918872475624,0.017015498131514,0.038635216653347],[0.010183644481003,0.010183644481003,0.0058755348436534,0.010339170694351,0.0073149777017534,0.0075591756030917,0.012034345418215],[0.019561149179935,0.019561149179935,0.036798965185881,0.029140489175916,0.0291514005512,0.016970975324512,0.0253866892308],[0.0028534086886793,0.0028534086886793,0.0013481815112755,0.0058964216150343,0.0033006470184773,0.0018656726460904,0.005618941038847],[0.0063073271885514,0.0063073271885514,0.0045462027192116,0.010365139693022,0.0076512494124472,0.010335328988731,0.014941040426493],[0.033661220222712,0.033661220222712,0.039589762687683,0.036186944693327,0.071009866893291,0.035569112747908,0.026428412646055],[0.0019375256961212,0.0019375256961212,0.0012629881966859,0.0043391864746809,0.0018298089271411,0.0011462797410786,0.002435308881104],[0.023062827065587,0.023062827065587,0.022321404889226,0.030390640720725,0.030207578092813,0.028675785288215,0.032695438712835],[0.0019375256961212,0.0019375256961212,0.0012629881966859,0.0043391864746809,0.0018298089271411,0.0011462797410786,0.002435308881104],[0.031378246843815,0.031378246843815,0.035135827958584,0.035186000168324,0.033826529979706,0.05373989790678,0.052966397255659],[0.0019375256961212,0.0019375256961212,0.0012629881966859,0.0043391864746809,0.0018298089271411,0.0011462797410786,0.002435308881104],[0.031378246843815,0.031378246843815,0.035135827958584,0.035186000168324,0.033826529979706,0.05373989790678,0.052966397255659],[0.0019375256961212,0.0019375256961212,0.0012629881966859,0.0043391864746809,0.0018298089271411,0.0011462797410786,0.002435308881104],[0.01157798897475,0.01157798897475,0.011384758166969,0.018848370760679,0.012692270800471,0.013191484846175,0.01690661162138],[0.010183644481003,0.010183644481003,0.0058755348436534,0.010339170694351,0.0073149777017534,0.0075591756030917,0.012034345418215],[0.022603875026107,0.022603875026107,0.018001781776547,0.022138079628348,0.02477041631937,0.023755015805364,0.022755838930607],[0.038924787193537,0.038924787193537,0.023082600906491,0.048486694693565,0.072061620652676,0.095677800476551,0.041510213166475],[0.010183644481003,0.010183644481003,0.0058755348436534,0.010339170694351,0.0073149777017534,0.0075591756030917,0.012034345418215],[0.034676723182201,0.034676723182201,0.039691116660833,0.025204978883266,0.030837003141642,0.018189964815974,0.02582111954689],[0.0019375256961212,0.0019375256961212,0.0012629881966859,0.0043391864746809,0.0018298089271411,0.0011462797410786,0.002435308881104],[0.056111969053745,0.056111969053745,0.067134208977222,0.042234148830175,0.030622810125351,0.035720609128475,0.029433626681566],[0.0045833084732294,0.0045833084732294,0.0027174670249224,0.0048443539999425,0.0031658185180277,0.0013434329302981,0.0045913867652416],[0.051353637129068,0.051353637129068,0.04918610304594,0.050211042165756,0.033918872475624,0.017015498131514,0.038635216653347],[0.010183644481003,0.010183644481003,0.0058755348436534,0.010339170694351,0.0073149777017534,0.0075591756030917,0.012034345418215],[0.019561149179935,0.019561149179935,0.036798965185881,0.029140489175916,0.0291514005512,0.016970975324512,0.0253866892308],[0.0028534086886793,0.0028534086886793,0.0013481815112755,0.0058964216150343,0.0033006470184773,0.0018656726460904,0.005618941038847],[0.038924787193537,0.038924787193537,0.023082600906491,0.0484866946"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "93565,0.072061620652676,0.095677800476551,0.041510213166475],[0.0019375256961212,0.0019375256961212,0.0012629881966859,0.0043391864746809,0.0018298089271411,0.0011462797410786,0.002435308881104],[0.053517259657383,0.053517259657383,0.059730984270573,0.026241784915328,0.047115460038185,0.026427395641804,0.029263831675053],[0.0045833084732294,0.0045833084732294,0.0027174670249224,0.0048443539999425,0.0031658185180277,0.0013434329302981,0.0045913867652416],[0.0028534086886793,0.0028534086886793,0.0013481815112755,0.0058964216150343,0.0033006470184773,0.0018656726460904,0.005618941038847],[0.065339758992195,0.065339758992195,0.046260997653008,0.033174198120832,0.041776467114687,0.041298326104879,0.051148656755686],[0.0063073271885514,0.0063073271885514,0.0045462027192116,0.010365139693022,0.0076512494124472,0.010335328988731,0.014941040426493],[0.029353685677052,0.029353685677052,0.019236953929067,0.017091820016503,0.028875024989247,0.032210290431976,0.03286237642169],[0.0063073271885514,0.0063073271885514,0.0045462027192116,0.010365139693022,0.0076512494124472,0.010335328988731,0.014941040426493],[0.038924787193537,0.038924787193537,0.023082600906491,0.048486694693565,0.072061620652676,0.095677800476551,0.041510213166475],[0.0019375256961212,0.0019375256961212,0.0012629881966859,0.0043391864746809,0.0018298089271411,0.0011462797410786,0.002435308881104],[0.032793577760458,0.032793577760458,0.050858743488789,0.032167989760637,0.023074785247445,0.028897270560265,0.035947322845459],[0.0045833084732294,0.0045833084732294,0.0027174670249224,0.0048443539999425,0.0031658185180277,0.0013434329302981,0.0045913867652416],[0.024624677374959,0.024624677374959,0.025626182556152,0.022476680576801,0.023303288966417,0.026778364554048,0.027257092297077],[0.0034229347947985,0.0034229347947985,0.0024786402937025,0.006160918623209,0.0034672704059631,0.0027901714202017,0.0061246887780726],[0.024624677374959,0.024624677374959,0.025626182556152,0.022476680576801,0.023303288966417,0.026778364554048,0.027257092297077],[0.0034229347947985,0.0034229347947985,0.0024786402937025,0.006160918623209,0.0034672704059631,0.0027901714202017,0.0061246887780726],[0.024624677374959,0.024624677374959,0.025626182556152,0.022476680576801,0.023303288966417,0.026778364554048,0.027257092297077],[0.0028534086886793,0.0028534086886793,0.0013481815112755,0.0058964216150343,0.0033006470184773,0.0018656726460904,0.005618941038847],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0],[0,0,0,0,0,0,0]]\t\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON:encode(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\t{\n",
       "  1 : 0.039528504014015\n",
       "  2 : 0.039528504014015\n",
       "  3 : 0.062131494283676\n",
       "  4 : 0.044466331601143\n",
       "  5 : 0.018017549067736\n",
       "  6 : 0.030503196641803\n",
       "  7 : 0.035669844597578\n",
       "}\n",
       "2\t{\n",
       "  1 : 0.034676723182201\n",
       "  2 : 0.034676723182201\n",
       "  3 : 0.039691116660833\n",
       "  4 : 0.025204978883266\n",
       "  5 : 0.030837003141642\n",
       "  6 : 0.018189964815974\n",
       "  7 : 0.02582111954689\n",
       "}\n",
       "3\t{\n",
       "  1 : 0.0019375256961212\n",
       "  2 : 0.0019375256961212\n",
       "  3 : 0.0012629881966859\n",
       "  4 : 0.0043391864746809\n",
       "  5 : 0.0018298089271411\n",
       "  6 : 0.0011462797410786\n",
       "  7 : 0.002435308881104\n",
       "}\n",
       "4\t{\n",
       "  1 : 0.056111969053745\n",
       "  2 : 0.056111969053745\n",
       "  3 : 0.067134208977222\n",
       "  4 : 0.042234148830175\n",
       "  5 : 0.030622810125351\n",
       "  6 : 0.035720609128475\n",
       "  7 : 0.029433626681566\n",
       "}\n",
       "5\t{\n",
       "  1 : 0.0045833084732294\n",
       "  2 : 0.0045833084732294\n",
       "  3 : 0.0027174670249224\n",
       "  4 : 0.0048443539999425\n",
       "  5 : 0.0031658185180277\n",
       "  6 : 0.0013434329302981\n",
       "  7 : 0.0045913867652416\n",
       "}\n",
       "6\t{\n",
       "  1 : 0.051353637129068\n",
       "  2 : 0.051353637129068\n",
       "  3 : 0.04918610304594\n",
       "  4 : 0.050211042165756\n",
       "  5 : 0.033918872475624\n",
       "  6 : 0.017015498131514\n",
       "  7 : 0.038635216653347\n",
       "}\n",
       "7\t{\n",
       "  1 : 0.010183644481003\n",
       "  2 : 0.010183644481003\n",
       "  3 : 0.0058755348436534\n",
       "  4 : 0.010339170694351\n",
       "  5 : 0.0073149777017534\n",
       "  6 : 0.0075591756030917\n",
       "  7 : 0.012034345418215\n",
       "}\n",
       "8\t{\n",
       "  1 : 0.019561149179935\n",
       "  2 : 0.019561149179935\n",
       "  3 : 0.036798965185881\n",
       "  4 : 0.029140489175916\n",
       "  5 : 0.0291514005512\n",
       "  6 : 0.016970975324512\n",
       "  7 : 0.0253866892308\n",
       "}\n",
       "9\t{\n",
       "  1 : 0.0028534086886793\n",
       "  2 : 0.0028534086886793\n",
       "  3 : 0.0013481815112755\n",
       "  4 : 0.0058964216150343\n",
       "  5 : 0.0033006470184773\n",
       "  6 : 0.0018656726460904\n",
       "  7 : 0.005618941038847\n",
       "}\n",
       "10\t{\n",
       "  1 : 0.0063073271885514\n",
       "  2 : 0.0063073271885514\n",
       "  3 : 0.0045462027192116\n",
       "  4 : 0.010365139693022\n",
       "  5 : 0.0076512494124472\n",
       "  6 : 0.010335328988731\n",
       "  7 : 0.014941040426493\n",
       "}\n",
       "11\t{\n",
       "  1 : 0.033661220222712\n",
       "  2 : 0.033661220222712\n",
       "  3 : 0.039589762687683\n",
       "  4 : 0.036186944693327\n",
       "  5 : 0.071009866893291\n",
       "  6 : 0.035569112747908\n",
       "  7 : 0.026428412646055\n",
       "}\n",
       "12\t{\n",
       "  1 : 0.0019375256961212\n",
       "  2 : 0.0019375256961212\n",
       "  3 : 0.0012629881966859\n",
       "  4 : 0.0043391864746809\n",
       "  5 : 0.0018298089271411\n",
       "  6 : 0.0011462797410786\n",
       "  7 : 0.002435308881104\n",
       "}\n",
       "13\t{\n",
       "  1 : 0.023062827065587\n",
       "  2 : 0.023062827065587\n",
       "  3 : 0.022321404889226\n",
       "  4 : 0.030390640720725\n",
       "  5 : 0.030207578092813\n",
       "  6 : 0.028675785288215\n",
       "  7 : 0.032695438712835\n",
       "}\n",
       "14\t{\n",
       "  1 : 0.0019375256961212\n",
       "  2 : 0.0019375256961212\n",
       "  3 : 0.0012629881966859\n",
       "  4 : 0.0043391864746809\n",
       "  5 : 0.0018298089271411\n",
       "  6 : 0.0011462797410786\n",
       "  7 : 0.002435308881104\n",
       "}\n",
       "15\t{\n",
       "  1 : 0.031378246843815\n",
       "  2 : 0.031378246843815\n",
       "  3 : 0.035135827958584\n",
       "  4 : 0.035186000168324\n",
       "  5 : 0.033826529979706\n",
       "  6 : 0.05373989790678\n",
       "  7 : 0.052966397255659\n",
       "}\n",
       "16\t{\n",
       "  1 : 0.0019375256961212\n",
       "  2 : 0.0019375256961212\n",
       "  3 : 0.0012629881966859\n",
       "  4 : 0.0043391864746809\n",
       "  5 : 0.0018298089271411\n",
       "  6 : 0.0011462797410786\n",
       "  7 : 0.002435308881104\n",
       "}\n",
       "17\t{\n",
       "  1 : 0.031378246843815\n",
       "  2 : 0.031378246843815\n",
       "  3 : 0.035135827958584\n",
       "  4 : 0.035186000168324\n",
       "  5 : 0.033826529979706\n",
       "  6 : 0.05373989790678\n",
       "  7 : 0.052966397255659\n",
       "}\n",
       "18\t{\n",
       "  1 : 0.0019375256961212\n",
       "  2 : 0.0019375256961212\n",
       "  3 : 0.0012629881966859\n",
       "  4 : 0.0043391864746809\n",
       "  5 : 0.0018298089271411\n",
       "  6 : 0.0011462797410786\n",
       "  7 : 0.002435308881104\n",
       "}\n",
       "19\t{\n",
       "  1 : 0.01157798897475\n",
       "  2 : 0.01157798897475\n",
       "  3 : 0.011384758166969\n",
       "  4 : 0.018848370760679\n",
       "  5 : 0.012692270800471\n",
       "  6 : 0.013191484846175\n",
       "  7 : 0.01690661162138\n",
       "}\n",
       "20\t{\n",
       "  1 : 0.010183644481003\n",
       "  2 : 0.010183644481003\n",
       "  3 : 0.0058755348436534\n",
       "  4 : 0.010339170694351\n",
       "  5 : 0.0073149777017534\n",
       "  6 : 0.0075591756030917\n",
       "  7 : 0.012034345418215\n",
       "}\n",
       "21\t{\n",
       "  1 : 0.022603875026107\n",
       "  2 : 0.022603875026107\n",
       "  3 : 0.018001781776547\n",
       "  4 : 0.022138079628348\n",
       "  5 : 0.02477041631937\n",
       "  6 : 0.023755015805364\n",
       "  7 : 0.022755838930607\n",
       "}\n",
       "22\t{\n",
       "  1 : 0.038924787193537\n",
       "  2 : 0.038924787193537\n",
       "  3 : 0.023082600906491\n",
       "  4 : 0.048486694693565\n",
       "  5 : 0.072061620652676\n",
       "  6 : 0.095677800476551\n",
       "  7 : 0.041510213166475\n",
       "}\n",
       "23\t{\n",
       "  1 : 0.010183644481003\n",
       "  2 : 0.010183644481003\n",
       "  3 : 0.0058755348436534\n",
       "  4 : 0.010339170694351\n",
       "  5 : 0.0073149777017534\n",
       "  6 : 0.0075591756030917\n",
       "  7 : 0.012034345418215\n",
       "}\n",
       "24\t{\n",
       "  1 : 0.034676723182201\n",
       "  2 : 0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       ".034676723182201\n",
       "  3 : 0.039691116660833\n",
       "  4 : 0.025204978883266\n",
       "  5 : 0.030837003141642\n",
       "  6 : 0.018189964815974\n",
       "  7 : 0.02582111954689\n",
       "}\n",
       "25\t{\n",
       "  1 : 0.0019375256961212\n",
       "  2 : 0.0019375256961212\n",
       "  3 : 0.0012629881966859\n",
       "  4 : 0.0043391864746809\n",
       "  5 : 0.0018298089271411\n",
       "  6 : 0.0011462797410786\n",
       "  7 : 0.002435308881104\n",
       "}\n",
       "26\t{\n",
       "  1 : 0.056111969053745\n",
       "  2 : 0.056111969053745\n",
       "  3 : 0.067134208977222\n",
       "  4 : 0.042234148830175\n",
       "  5 : 0.030622810125351\n",
       "  6 : 0.035720609128475\n",
       "  7 : 0.029433626681566\n",
       "}\n",
       "27\t{\n",
       "  1 : 0.0045833084732294\n",
       "  2 : 0.0045833084732294\n",
       "  3 : 0.0027174670249224\n",
       "  4 : 0.0048443539999425\n",
       "  5 : 0.0031658185180277\n",
       "  6 : 0.0013434329302981\n",
       "  7 : 0.0045913867652416\n",
       "}\n",
       "28\t{\n",
       "  1 : 0.051353637129068\n",
       "  2 : 0.051353637129068\n",
       "  3 : 0.04918610304594\n",
       "  4 : 0.050211042165756\n",
       "  5 : 0.033918872475624\n",
       "  6 : 0.017015498131514\n",
       "  7 : 0.038635216653347\n",
       "}\n",
       "29\t{\n",
       "  1 : 0.010183644481003\n",
       "  2 : 0.010183644481003\n",
       "  3 : 0.0058755348436534\n",
       "  4 : 0.010339170694351\n",
       "  5 : 0.0073149777017534\n",
       "  6 : 0.0075591756030917\n",
       "  7 : 0.012034345418215\n",
       "}\n",
       "30\t{\n",
       "  1 : 0.019561149179935\n",
       "  2 : 0.019561149179935\n",
       "  3 : 0.036798965185881\n",
       "  4 : 0.029140489175916\n",
       "  5 : 0.0291514005512\n",
       "  6 : 0.016970975324512\n",
       "  7 : 0.0253866892308\n",
       "}\n",
       "31\t{\n",
       "  1 : 0.0028534086886793\n",
       "  2 : 0.0028534086886793\n",
       "  3 : 0.0013481815112755\n",
       "  4 : 0.0058964216150343\n",
       "  5 : 0.0033006470184773\n",
       "  6 : 0.0018656726460904\n",
       "  7 : 0.005618941038847\n",
       "}\n",
       "32\t{\n",
       "  1 : 0.038924787193537\n",
       "  2 : 0.038924787193537\n",
       "  3 : 0.023082600906491\n",
       "  4 : 0.048486694693565\n",
       "  5 : 0.072061620652676\n",
       "  6 : 0.095677800476551\n",
       "  7 : 0.041510213166475\n",
       "}\n",
       "33\t{\n",
       "  1 : 0.0019375256961212\n",
       "  2 : 0.0019375256961212\n",
       "  3 : 0.0012629881966859\n",
       "  4 : 0.0043391864746809\n",
       "  5 : 0.0018298089271411\n",
       "  6 : 0.0011462797410786\n",
       "  7 : 0.002435308881104\n",
       "}\n",
       "34\t{\n",
       "  1 : 0.053517259657383\n",
       "  2 : 0.053517259657383\n",
       "  3 : 0.059730984270573\n",
       "  4 : 0.026241784915328\n",
       "  5 : 0.047115460038185\n",
       "  6 : 0.026427395641804\n",
       "  7 : 0.029263831675053\n",
       "}\n",
       "35\t{\n",
       "  1 : 0.0045833084732294\n",
       "  2 : 0.0045833084732294\n",
       "  3 : 0.0027174670249224\n",
       "  4 : 0.0048443539999425\n",
       "  5 : 0.0031658185180277\n",
       "  6 : 0.0013434329302981\n",
       "  7 : 0.0045913867652416\n",
       "}\n",
       "36\t{\n",
       "  1 : 0.0028534086886793\n",
       "  2 : 0.0028534086886793\n",
       "  3 : 0.0013481815112755\n",
       "  4 : 0.0058964216150343\n",
       "  5 : 0.0033006470184773\n",
       "  6 : 0.0018656726460904\n",
       "  7 : 0.005618941038847\n",
       "}\n",
       "37\t{\n",
       "  1 : 0.065339758992195\n",
       "  2 : 0.065339758992195\n",
       "  3 : 0.046260997653008\n",
       "  4 : 0.033174198120832\n",
       "  5 : 0.041776467114687\n",
       "  6 : 0.041298326104879\n",
       "  7 : 0.051148656755686\n",
       "}\n",
       "38\t{\n",
       "  1 : 0.0063073271885514\n",
       "  2 : 0.0063073271885514\n",
       "  3 : 0.0045462027192116\n",
       "  4 : 0.010365139693022\n",
       "  5 : 0.0076512494124472\n",
       "  6 : 0.010335328988731\n",
       "  7 : 0.014941040426493\n",
       "}\n",
       "39\t{\n",
       "  1 : 0.029353685677052\n",
       "  2 : 0.029353685677052\n",
       "  3 : 0.019236953929067\n",
       "  4 : 0.017091820016503\n",
       "  5 : 0.028875024989247\n",
       "  6 : 0.032210290431976\n",
       "  7 : 0.03286237642169\n",
       "}\n",
       "40\t{\n",
       "  1 : 0.0063073271885514\n",
       "  2 : 0.0063073271885514\n",
       "  3 : 0.0045462027192116\n",
       "  4 : 0.010365139693022\n",
       "  5 : 0.0076512494124472\n",
       "  6 : 0.010335328988731\n",
       "  7 : 0.014941040426493\n",
       "}\n",
       "41\t{\n",
       "  1 : 0.038924787193537\n",
       "  2 : 0.038924787193537\n",
       "  3 : 0.023082600906491\n",
       "  4 : 0.048486694693565\n",
       "  5 : 0.072061620652676\n",
       "  6 : 0.095677800476551\n",
       "  7 : 0.041510213166475\n",
       "}\n",
       "42\t{\n",
       "  1 : 0.0019375256961212\n",
       "  2 : 0.0019375256961212\n",
       "  3 : 0.0012629881966859\n",
       "  4 : 0.0043391864746809\n",
       "  5 : 0.0018298089271411\n",
       "  6 : 0.0011462797410786\n",
       "  7 : 0.002435308881104\n",
       "}\n",
       "43\t{\n",
       "  1 : 0.032793577760458\n",
       "  2 : 0.032793577760458\n",
       "  3 : 0.050858743488789\n",
       "  4 : 0.032167989760637\n",
       "  5 : 0.023074785247445\n",
       "  6 : 0.028897270560265\n",
       "  7 : 0.035947322845459\n",
       "}\n",
       "44\t{\n",
       "  1 : 0.0045833084732294\n",
       "  2 : 0.0045833084732294\n",
       "  3 : 0.0027174670249224\n",
       "  4 : 0.0048443539999425\n",
       "  5 : 0.0031658185180277\n",
       "  6 : 0.0013434329302981\n",
       "  7 : 0.0045913867652416\n",
       "}\n",
       "45\t{\n",
       "  1 : 0.024624677374959\n",
       "  2 : 0.024624677374959\n",
       "  3 : 0.025626182556152\n",
       "  4 : 0.022476680576801\n",
       "  5 : 0.023303288966417\n",
       "  6 : 0.026778364554048\n",
       "  7 : 0.027257092297077\n",
       "}\n",
       "46\t{\n",
       "  1 : 0.0034229347947985\n",
       "  2 : 0.0034229347947985\n",
       "  3 : 0.0024786402937025\n",
       "  4 : 0.006160918623209\n",
       "  5 : 0.0034672704059631\n",
       "  6 : 0.0027901714202017\n",
       "  7 : 0.0061246887780726\n",
       "}\n",
       "47\t{\n",
       "  1 : 0.024624677374959\n",
       "  2 : 0.024624677"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "374959\n",
       "  3 : 0.025626182556152\n",
       "  4 : 0.022476680576801\n",
       "  5 : 0.023303288966417\n",
       "  6 : 0.026778364554048\n",
       "  7 : 0.027257092297077\n",
       "}\n",
       "48\t{\n",
       "  1 : 0.0034229347947985\n",
       "  2 : 0.0034229347947985\n",
       "  3 : 0.0024786402937025\n",
       "  4 : 0.006160918623209\n",
       "  5 : 0.0034672704059631\n",
       "  6 : 0.0027901714202017\n",
       "  7 : 0.0061246887780726\n",
       "}\n",
       "49\t{\n",
       "  1 : 0.024624677374959\n",
       "  2 : 0.024624677374959\n",
       "  3 : 0.025626182556152\n",
       "  4 : 0.022476680576801\n",
       "  5 : 0.023303288966417\n",
       "  6 : 0.026778364554048\n",
       "  7 : 0.027257092297077\n",
       "}\n",
       "50\t{\n",
       "  1 : 0.0028534086886793\n",
       "  2 : 0.0028534086886793\n",
       "  3 : 0.0013481815112755\n",
       "  4 : 0.0058964216150343\n",
       "  5 : 0.0033006470184773\n",
       "  6 : 0.0018656726460904\n",
       "  7 : 0.005618941038847\n",
       "}\n",
       "51\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "52\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "53\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "54\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "55\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "56\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "57\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "58\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "59\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "60\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "61\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "62\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "63\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "64\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "65\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "66\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "67\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "68\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "69\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "70\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "71\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "72\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "73\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "74\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "75\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "76\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "77\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "78\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "79\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "80\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "81\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "82\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "83\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "84\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "85\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "86\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "87\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "88\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "89\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "90\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "91\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "92\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "93\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "94\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "95\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "96\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "97\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "98\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n",
       "99\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "100\t{\n",
       "  1 : 0\n",
       "  2 : 0\n",
       "  3 : 0\n",
       "  4 : 0\n",
       "  5 : 0\n",
       "  6 : 0\n",
       "  7 : 0\n",
       "}\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key, value in pairs(tt) do\n",
    "    print(key, value)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nil\t\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decoder.cell.forwardnodes[56].data.mapindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#decoder.cell.fg.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
